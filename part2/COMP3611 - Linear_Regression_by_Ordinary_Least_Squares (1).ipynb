{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Ltmn1AU5QW-"
   },
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons Licence\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" /></a><br /><span xmlns:dct=\"http://purl.org/dc/terms/\" property=\"dct:title\">COMP3611M - Linear Regression by Ordinary Least Squares</span> by <span xmlns:cc=\"http://creativecommons.org/ns#\" property=\"cc:attributionName\">Marc de Kamps and University of Leeds</span> is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kCZy4O25QXA"
   },
   "source": [
    "## Linear Regression\n",
    "\n",
    "### Ordinary Least Squares\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "The purpose of this notebook is to introduce the formulae relevant to linear regression, using least squares, and Bayesian linear regression and discuss its implementation in Python. In the first part of the notebook, we discuss Ordinary Least Squares (OLS) where we attempt to fit polynomials to data. We will introduce the design matrix and show how it can be populated by a data file containg CVS separated data points. We will fit different polynomials to the data using OLS. *This material should be familar from the Data Science module.*  In the second part, we will apply Bayesian linear regression to the same problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ih6BC_nB5QXB"
   },
   "source": [
    "#### The Design Matrix\n",
    "\n",
    "Assume we want to fit a polynomial to a data set. We will consider one dimensional measurement values first. A data set then consists of a set of pairs of numbers: $ \\cup^N_{i=1} \\left\\{ (x_i, y_i) \\right\\}$.\n",
    "\n",
    "A third degree polynomial for example has the general\n",
    "form:\n",
    "$$\n",
    "f(x) = a + bx + cx^2 + dx^3\n",
    "$$\n",
    "The objective of linear regression is to fit a polynomial of a given degree to a data set. This means that we want to find numbers $a, b, c, d$ such that in some sense when we apply $f(x)$ to data point $x_i$, the value will\n",
    "be 'as close as possible' to the actual measured value $y_i$. Clearly the higher degree of the polynomial, the\n",
    "more flexible our fit can be. This has to be balanced against the possibility of *overfitting*.\n",
    "\n",
    "Ordinary Least Squares aims to minimise the sum of the squared residuals where the residual for data point $i$, $(x_i, y_i)$, is defined as:\n",
    "$$\n",
    "  r_i = y_i - f(x_i)\n",
    "$$\n",
    "Clearly the residu is dependent on the choice we've made for $a, b, c$ and $d$ and we want to find values such\n",
    "that\n",
    "$$\n",
    "\\sum_i r^2_i\n",
    "$$\n",
    "is minimal. The solution to this is well known and can be formulated in terms of the *design matrix*.\n",
    "\n",
    "We rewrite our polynomial in a more general notation:\n",
    "$$\n",
    "f(x) = {\\bf w}^T{\\bf \\phi}(x)\n",
    "$$\n",
    "\n",
    "This notation is shorthand for:\n",
    "$$\n",
    "f(x) = \\sum^M_{i=1} w_i \\phi^i(x)\n",
    "$$\n",
    "Here ${\\bf \\phi}$ is the vector:\n",
    "$$\n",
    "{\\bf \\phi}(x) = \\left( \\begin{array}{l} 1 \\\\ x \\\\ x^2 \\\\ x^3 \\end{array} \\right)\n",
    "$$\n",
    "whilst\n",
    "$$\n",
    "{\\bf w} = \\left( \\begin{array}{l} a \\\\ b \\\\ c\\\\ d\\end{array} \\right)\n",
    "$$\n",
    "so that the matrix product ${\\bf w}^T {\\bf \\phi}(x)$ works out as:\n",
    "$$\n",
    "{\\bf w}^T{\\bf \\phi}(x) = \\left( \\begin{array}{llll} a & b & c & d \\end{array} \\right) \\left( \\begin{array}{l} 1 \\\\ x \\\\ x^2 \\\\ x^3 \\end{array} \\right) = a + bx + cx^2 + d x^3\n",
    "$$\n",
    "In this notebook we main the convention that $\\phi^i$ is the $i$-th component of $N \\times 1$ matrix  ${\\bf \\phi}$, whereas\n",
    "$w_j$ is the $j$-th component of the $1 \\times N$ matrix ${\\bf w}^T$.\n",
    "\n",
    "This notation has the advantage that it generalises to polynomials of any dimension. The degree of the polynomial that we regress to is $M$, in the example thus far $M=4$, but the notation extends trivially to any dimension.\n",
    "\n",
    "For each data point we have a column vector ${\\bf \\phi}(x_n)$. The *desigin matrix* is an $N \\times M$ matrix whose\n",
    "elements are given by $\\Phi_{nj} = \\phi_j(x_n)$ so that:\n",
    "\\begin{equation}\n",
    "\\Phi = \\left( \\begin{array}{cccc} \\phi_0(x_1) & \\phi_1(x_1) & \\cdots & \\phi_{M-1}(x_1) \\\\\n",
    "                                  \\phi_0(x_2) & \\phi_1(x_2) & \\cdots & \\phi_{M-1}(x_2) \\\\\n",
    "                                  \\vdots      & \\vdots      &        & \\vdots          \\\\\n",
    "                                  \\phi_0(x_N) & \\phi_1(x_N) & \\cdots & \\phi_{M-1}(x_N) \\end{array}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "As an example, we will read in a dataset that we want to fit by a linear function ($M=2$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jrewYHOG5QXC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2)\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [8,4]\n",
    "plt.rcParams['figure.dpi']     = 200\n",
    "\n",
    "import numpy as  np\n",
    "\n",
    "with open('linnoise.dat') as f:\n",
    "    lines = f.readlines()\n",
    "    xs = [float(x.split(',')[0]) for x in lines]\n",
    "    t  = [float(y.split(',')[1]) for y in lines]\n",
    "\n",
    "\n",
    "Phi = np.array([np.power(xs,0),np.power(xs,1)]).T # for every degree of the polynomial we add a column to the design matrix\n",
    "print(Phi.shape)\n",
    "print(len(xs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RW1SYHVR5QXE"
   },
   "source": [
    "The OLS solution is formally given by:\n",
    "$$\n",
    "{\\bf w}_{ML} = ({\\bf \\Phi}^T{\\bf \\Phi})^{-1} {\\bf \\Phi}^T {\\bf t}\n",
    "$$\n",
    "\n",
    "$t$ is the vector of values $y_i$, so an $N \\times 1$ matrix:\n",
    "$$\n",
    "{\\bf t} = \\left( \\begin{array}{c} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{array} \\right)\n",
    "$$\n",
    "The quantity ${\\bf \\Phi}^{\\dagger} \\equiv ({\\bf \\Phi}^T{\\bf \\Phi})^{-1}{\\bf \\Phi}^T$ is known as the *Moore-Penrose pseudo-inverse*.\n",
    "It is the product  of an $M \\times N$ and a $N  \\times M$ matrix, so an $M \\times M$ matrix, which\n",
    "is inverted, and thereby remains an $M \\times  M$ matrix, which is right multiplied by an $M \\times N$ matrix,\n",
    "so an $M \\times N$ matrix. When this is applied to $N \\times 1$ matrix (vector) ${\\bf t}$, an $M \\times 1$\n",
    "matrix results, which contains the coefficients of the OLS polynomial.\n",
    "\n",
    "This is *not* the recommend numerical solution which uses SVD decomposition to avoid numerical instabilities, but it often works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "keeMIc-q5QXE",
    "outputId": "e6831a94-b410-478d-b34a-a90bde607457"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "(2, 2000)\n",
      "[-0.70176791  0.28951715]\n"
     ]
    }
   ],
   "source": [
    "m=Phi.T.dot(Phi)\n",
    "print(m.shape)\n",
    "print(Phi.T.shape)\n",
    "\n",
    "Phi_dagger = np.linalg.inv(Phi.T.dot(Phi)).dot(Phi.T)\n",
    "w = Phi_dagger.dot(t)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R3ZPQN1L5QXF"
   },
   "source": [
    "This is the *MLE* of the weights. It is a single point in weight space, i.e. it identifies a unique line as best estimate.\n",
    "\n",
    "\n",
    "It is straightforward to examine the results of the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eTBXkmk65QXF",
    "outputId": "333c4a65-643f-4713-9659-dd2232a8bf0e"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_min = np.min(xs)\n",
    "x_max = np.max(xs)\n",
    "\n",
    "x_fit = np.linspace(x_min,x_max,100)\n",
    "y_fit = w[1]*x_fit + w[0]\n",
    "plt.plot(xs,t,'.')\n",
    "plt.plot(x_fit,y_fit,'r-')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.savefig('linres.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3YBzjLnh5QXG"
   },
   "source": [
    "Fitting a third order polynomial is a straightforward extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5cWRdQf5QXG"
   },
   "outputs": [],
   "source": [
    "with open('cubnoise.dat') as f:\n",
    "    lines = f.readlines()\n",
    "    xs = [float(x.split(',')[0]) for x in lines]\n",
    "    t  = [float(y.split(',')[1]) for y in lines]\n",
    "\n",
    "n_degree = 4 # a cubic polynomial has four coefficients\n",
    "Phi = np.array([np.power(xs,i) for i in range(n_degree)]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24JGSSAx5QXG",
    "outputId": "c00e7ea1-77d1-4740-808a-54979dda96e8"
   },
   "outputs": [],
   "source": [
    "Phi_dagger = np.linalg.inv(Phi.T.dot(Phi)).dot(Phi.T)\n",
    "w = Phi_dagger.dot(t)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HSli4d965QXH",
    "outputId": "b87cb204-2771-4082-edd8-a731fa9ba4b9"
   },
   "outputs": [],
   "source": [
    "x_min = np.min(xs)\n",
    "x_max = np.max(xs)\n",
    "\n",
    "x_fit = np.linspace(x_min,x_max,100)\n",
    "\n",
    "# this is what we want to do, but it is cluncky, you can uncomment to see it works, but it's better to use polyval\n",
    "#y_fit = w[3]*x_fit*x_fit*x_fit + w[2]*x_fit*x_fit + w[1]*x_fit + w[0]\n",
    "y_fit = np.polyval(np.flip(w),x_fit) # polyval applies w[0] to x**3, so w needs to be reversed\n",
    "\n",
    "plt.plot(xs,t,'.')\n",
    "plt.plot(x_fit,y_fit,'r-')\n",
    "plt.savefig('cubic.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nsvsa2CI5QXH"
   },
   "source": [
    "The results, by visual inspection, appear quite reasonable and the coefficents (weights) are neither small nor large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MeJvs31E5QXH"
   },
   "source": [
    "### Overfitting\n",
    "\n",
    "Let us revisit the linear dataset and fit a high order polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZB880sIK5QXH",
    "outputId": "212c99f2-d1d0-4356-c91a-66b417e5a9a2"
   },
   "outputs": [],
   "source": [
    "with open('linnoise.dat') as f:\n",
    "    lines = f.readlines()\n",
    "    xs = [float(x.split(',')[0]) for x in lines]\n",
    "    t  = [float(y.split(',')[1]) for y in lines]\n",
    "\n",
    "n_degree = 2 # fit a polynomial of a given order\n",
    "\n",
    "# just work on 10 points to better expose the effects of overfitting\n",
    "n_red = 10\n",
    "xsred = xs[:n_red]\n",
    "tred  = t[:n_red]\n",
    "\n",
    "Phi = np.array([np.power(xsred,i) for i in range(n_degree)]).T\n",
    "\n",
    "Phi_dagger = np.linalg.inv(Phi.T.dot(Phi)).dot(Phi.T)\n",
    "w = Phi_dagger.dot(tred)\n",
    "print(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5wvSXMDr5QXI",
    "outputId": "9cf38554-b4d7-4950-f03d-e3f8e4fd873e"
   },
   "outputs": [],
   "source": [
    "x_min = np.min(xsred)\n",
    "x_max = np.max(xsred)\n",
    "\n",
    "\n",
    "x_fit = np.linspace(x_min,x_max,100)\n",
    "y_fit = np.polyval(np.flip(w),x_fit)\n",
    "\n",
    "\n",
    "plt.plot(xsred,tred,'.')\n",
    "plt.plot(x_fit,y_fit,'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1IHFyr1P5QXI"
   },
   "source": [
    "Note how the linear fit is still appropriate, and approximately gives the same results on a small number of points. Now let's try the same for a high degree polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kDhQemv5QXI",
    "outputId": "cec2834c-99e3-4b45-a35d-12c50a1ffb7e"
   },
   "outputs": [],
   "source": [
    "n_degree = 12 # fit a polynomial of a given order\n",
    "Phi = np.array([np.power(xsred,i) for i in range(n_degree)]).T\n",
    "\n",
    "Phi_dagger = np.linalg.inv(Phi.T.dot(Phi)).dot(Phi.T)\n",
    "w = Phi_dagger.dot(tred)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pW7F7laL5QXI",
    "outputId": "c0278dfe-064e-4292-d871-377db5cd69ba"
   },
   "outputs": [],
   "source": [
    "x_fit = np.linspace(x_min,x_max,100)\n",
    "y_fit = np.polyval(np.flip(w),x_fit)\n",
    "\n",
    "\n",
    "plt.plot(xsred,tred,'.')\n",
    "plt.plot(x_fit,y_fit,'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LG8hOr175QXI"
   },
   "source": [
    "The fit is good in the sense that it hits all data points with considerable accuracy, but the weights are huge, and the resulting polynomial is not representative of the underlying data at all. Note the large value of the coeefficients, suggesting that the higher order terms have a large influence in constraining the polynomial to the data. But large higher order terms in a polynomial imply a wildly fluctuating polynomial. This is quite visible!\n",
    "\n",
    "Let's zoom in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8vA3DXA5QXI",
    "outputId": "e9ce5398-6533-4a1c-b36e-32884558a0f3"
   },
   "outputs": [],
   "source": [
    "plt.plot(xsred,tred,'.')\n",
    "plt.plot(x_fit,y_fit,'r-')\n",
    "plt.xlim([x_min,x_max+1])\n",
    "plt.ylim([-1.,5.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCol2aP95QXJ"
   },
   "source": [
    "The problem can be alleviated by using more data but:\n",
    "1. There may not be more data. Here there is, but in general you may not be able to rerun an experiment or you will have to use existing study data. The central problem is machine learning and statistics is to make do with the data you have.\n",
    "2. Even if you have more data, the resulting model is not necessarily better. We illustrate this below.\n",
    "\n",
    "If the underlying deterministic process generates data according to a linear deterministic model, and the variability of the data is mainly due to noise, the higher order terms of the fit still adapt more to the noise than to the data, but the need to accommodate the larger dataset reduces their importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HuqlIX3R5QXJ",
    "outputId": "3162625b-77c7-4df9-994d-f8f5b4d088d5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_degree = 12 # fit a polynomial of a given order\n",
    "Phi = np.array([np.power(xs,i) for i in range(n_degree)]).T\n",
    "\n",
    "Phi_dagger = np.linalg.inv(Phi.T.dot(Phi)).dot(Phi.T)\n",
    "w = Phi_dagger.dot(t)\n",
    "\n",
    "x_fit = np.linspace(x_min,x_max,100)\n",
    "y_fit = np.polyval(np.flip(w),x_fit)\n",
    "\n",
    "plt.plot(xs,t,'.')\n",
    "plt.plot(x_fit,y_fit,'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-dsQG2f5QXJ"
   },
   "source": [
    "The increased amount of data has *regularised* the polynomial, i.e. reduced the contribution of the higher order terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yaLdjotO5QXJ"
   },
   "source": [
    "### Linear Regression on Other Basis Functions\n",
    "\n",
    "We can create a number of Gaussian functions:\n",
    "$\\mu = 0, 1, \\cdots, 10$ and use them as basis functions instead of polynomials.\n",
    "\n",
    "The basis vectors $\\phi$, then become:\n",
    "$$\n",
    "\\boldsymbol{\\phi}(x_i) = \\left( \\begin{array}{c} 1 \\\\ \\mathcal{N}(x_i - 0, 1) \\\\ \\mathcal{N}(x_i - 1,1) \\\\ \\vdots \\\\ \\mathcal{N}(x_i - 10,1) \\end{array} \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FyH9XCa75QXJ",
    "outputId": "7be03a7a-f974-47b8-b65c-bf8980d01041"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "with open('gaussnoise.dat') as f:\n",
    "    lines = f.readlines()\n",
    "    xs = [float(x.split(',')[0]) for x in lines]\n",
    "    t  = [float(y.split(',')[1]) for y in lines]\n",
    "\n",
    "Phi = np.array([norm.pdf(xs,i,1.0) for i in range(10)]).T\n",
    "\n",
    "Phi_dagger = np.linalg.inv(Phi.T.dot(Phi)).dot(Phi.T)\n",
    "w = Phi_dagger.dot(t)\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1oNXlNuL5QXJ",
    "outputId": "da3d58a8-8758-45cc-b754-008cddbc9ded"
   },
   "outputs": [],
   "source": [
    "plt.plot(xs,t,'.')\n",
    "x = np.linspace(0,10,200)\n",
    "l=[w[i]*norm.pdf(x,i,1.0) for i in range(len(w))]\n",
    "for g in l:\n",
    "    plt.plot(x,g,'y-')\n",
    "tot=sum(l)\n",
    "plt.plot(x,tot,'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6qwuEAZ5QXK"
   },
   "source": [
    "The plot above shows all basis functions multiplied by their regression coefficient (yellow) and their sum (red). In this particular case, Gaussian functions provide an adequate set of basis functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vl9qE7wK5QXK"
   },
   "source": [
    "**Exercise 1**\n",
    "\n",
    "Research the linear regressor from scitkit-learn:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "\n",
    "Adapt the example to perform the fit of *linnoise.dat*. What are the functions for setting the weight; training and evaluating the goodness of fit?\n",
    "\n",
    "Do the coefficients compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fpw30Pnk5QXK"
   },
   "source": [
    "**Exercise 2**\n",
    "\n",
    "The webpage describing *scikit-learn*'s LinearRegression object does not mention basis functions. Is this a problem? Can you use this object to fit the data from *cubnoise.dat*? If so, do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SCtC8SxE5QXK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
